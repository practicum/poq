
In order to gauge whether it is worthwhile to build {{such a tool}} (put wording that ties in to preceding sections), it will be helpful to know how it might be applied in practice.  Therefore, in this section, we will imagine that a SQL verification tool exists, and we will examine the ways in which it could be applied.

The imagined tool will parse SQL code, parse DDL code, and also parse table-size and tuple-membership assertions.  The tool will then determine whether the assertions are valid for the given database schema.  If the given SQL and DDL admit any database state that would also cause a table-size or tuple-membership assertion to fail, then the tool will output one or more sample states in demonstration of these assertion violations.

Since the tool provides formal proof or refutation of assertion validity, it would seem that an obvious setting for the tool would be in critical systems development.  Critical systems have historically been the biggest adopters of formal methods in software.  Projects typically labeled as critical systems are flight control software and systems that regulate nuclear power plants.  Such projects, however, typically do *not* include SQL code.

However, there are other types of projects that might not be labeled critical but nonetheless are considered high-stakes projects, and that may even have a legal requirement or contractual obligation to achieve some formal certification of system correctness.  This tier of high-risk software includes financial applications and medical record-keeping systems.  These types of systems do tend to use a moderate to large amount of SQL code.

Unfortunately, our imagined SQL assertion-checker tool is probably still not sufficient to *certify* such financial and medical applications.  The reason is that even if the SQL code is verified, bugs in the underlying RDB system could still cause query results that violate the tested assertions.  The only way to rely on our SQL verification tool for complete system certification would be to guarantee that the verified SQL code will in turn only be executed on a verified RDB.  Inroads are being made into verified RDB systems [], but no such commercial-grade RDB yet exists.

Therefore, our imagined formal tool does not seem immediately applicable to high-stakes settings where formal methods currently have traction.  Understandably, then, our tool should be met with skepticism were this its only intended use.  Thankfully, as later chapters of this thesis will show, a SQL verfication tool can quite aptly perform duties that look more like today's testing and debugging techniques than like formal certification.

To understand how such a tool can be applied to a debugging objective, take the example in Figure ___. (do setup of what that code is).

A manager has recieved a summary report listing "<part-a, TOOHEAVY>, <part-a, TOOHEAVY>."  The manager accordingly reports that there is a bug in report generation that causes the same product complaint to appear twice in the same report.  A software developer is assigned to diagnose this bug.  The software developer runs the report query against several test database instances but is unable to generate a report containing a duplicate product complaint.  At this point in the diagnosis it is helpful to apply the SQL assertion-checker tool.  In order to "debug" using the tool, the developer will annotate the query with an assertion "NOT [ member() and member() ]" and the tool will return a counterexample like "       ."  In this way, the tool finds a scenario that reproduces the bug whereas the developer had failed to do so by trial and error.  At this point, the developer might notice that in the counterexample, the product id corresponding to the duplicate complaint is an id that appears in *both* base tables.  To determine whether that is the *only* way the duplication can occur, the original debugging assertion can be augmented like so: _______________.  With the augmented assertion, no counterexample is produced.  Therefore, the developer can conclude that the bug's root cause is that the query fails to handle the case when a given product id (-complaint pair) is present in both TABLE-BLA and TABLE-ZDR.  Such a bug may have been introduced by a developer that assumed the set of customer complaint codes was distinct from the set of internal employee complaint codes.  A debugging investigation is successfully completed with the aid of the SQL assertion-checker tool.

The foregoing example is unrealistic in one small way.  The only contrivance is the implicit idea that Query XYZ (name matching figure) is too daunting for a developer to diagnose without tool support.  In reality, any reasonably skilled SQL user should be able to diagnose this particular bug without difficulty.  However, the example is designed to show the feasibility and practicality of the described *workflow*, regardless of the size and complexity level of any individual query.  The example query is small to simplify discussion.  It should be clear that this tool-supported workflow is equally applicable for diagnosing queries involving more than two base tables and more than OOO (count them from figure) SQL operations.  Each developer will differ in deciding which queries can be tackled manually and which queries call for tool-supported debugging.  The same state of affairs exists with imperative debugger tools; some developers can spot certain bugs rapidly by manual code inspection, where other developers do not grasp the bug's full nature until stepping through the code with a debug tool.  The important thing is to have tools available when they are needed.  Step-through debuggers are readily available for imperative code.  Tools for debugging relational SQL are currently non-existent.

The term *step-through debugging* in imperative languages refers to executing consecutive statements one at a time, stopping after each statement to observe the system state.  The all-at-once nature of a SQL statement render the usual step-through approach inapplicable.  To debug a SQL statement, one might apply a novel definition of *step-through debugging* that hinges upon stepping through *states* rather than statements.  With SQL statements, one might prefer to systematically step through different isomorphic database states, pausing with each new state to observe how the SQL statement performs for the given state.

The preceding "product..." example shows that our imagined tool can play the role of a debugger.  The final goal of this section is to show how the tool can function in place of a more traditional testing system.

On many software projects, the development team adheres to a policy of adding a new test to a test suite each time a new bug is diagnosed.  In the "" scenario just described, the bug would be fixed, and then a related test script would be created.  The test script would loads data into a database instance, run the current version of the SQL query, then check whether or not any product complaint appears more than once in the result.  The original SQL query would have failed such a test.  Once the SQL code is repaired, scheduling the test script to be executed at regular intervals will protect the development team from ever accidentally reintroducing the same bug later on.  These tests are commonly called regression tests.

An assertion-checker tool can participate in regression testing just as it did for debugging.  All that is required is to enable the assertion-checker to automatically reverify "_______________" against the current SQL and DDL at regular intervals.  This can replace the running of test scripts at regular intervals.  This is desirable because it circumvents the need to repeatedly setup and destroy test database instances.  Furthermore, it is easier to write a concise assertion than it is to write a multi-line imperative script that includes database creation and table population and so forth.

Of course, the only reason regression tests have a special name is to indicate the cause that motivated their inclusion in the test suite.  Regression tests are created reactively, in response to newly-discovered bugs.  However, there is nothing unique in the structure or usage of regression tests that would differentiate them from proactively created tests.  Therefore, if our imagined SQL verification tool can replace regression tests, then it can replace other tests as well.  Accordingly, the last example of this section deals with proactively written tests.

One setting in which tests are inarguably proactive is in Test-Driven Development (TDD).  When applying TDD, a developer will not write application code until he/she has deeply considered how the output of the planned new code will look in a variety of circumstances.  Based on these considerations, tests are created that will detect whether or not the final application code matches the expected outcomes.  Also, a small piece of naive "stub" code is created that can be exercised by the test.  The stub code is initially designed to fail.  Once the test and stub are runnable, then an actual implementation is crafted to replace the stub code.  By applying TDD, the developer ensures that every part of the application has one or more tests enforcing the continued proper behavior the code during all future evolutions of the codebase.  Proactive test coverage such as this can be particularly important in applications that must continue to remain reverse-compatible with historically supported input formats or historically supported guarantees of legacy APIs.

Figure ____ will help illustrate how a SQL verification tool could be used in a TDD-like development style.






======================================

Figure {} illustrates a sample reporting task that is common in databases for customer purchase histories.  The database contains base tables for customer records and for linking customers to purchases.  The reporting task can be summarized as "produce one summary row per customer to tally the total count of orders ever placed by that customer."

In order to implement the report using a TDD methodology, the developer will consider future outcomes before writing code.  In this case, the developer will visualize what the proper output of the report will look like.  Figure {} shows a visualization, but the figure also includes sample SQL code.  At early stages in a TDD approach to this task, the visualized output would exist, but the code would not.

Using the feature requirements and a tentative idea of future outcomes, the developer will then devise a test that must necessarily be passed by any proposed implementation of the feature.  In the case of the present example, one such test would ensure that the row count of the output matches the row count of the Customer table.  Testing whether the two row counts are equal can be accomplished using __(assertion formula)____(what we last called the tool)______.

To keep this example simple, we will assume that each customer in the table has at least one ____.  This way, we know that no customers will be excluded ....  By eliminating the possiblity ______, we focus on other errors, such as ____.

Once the test has been devised, the next step is to create a very simple stub implementation to demonstrate that the chosen test framework is operational.  The stub implementation intentionally seeks to provoke a "fail" result from the test.  The goal of this step is to establish that the test is runnable and to demonstrate that the test indeed detects a problem when an implementation is flawed.  One sample stub implementation is shown in figure {}.  The stub is adequate for provoking test failure, because the output contains a larger row count than the Customer base table.

The final task for the developer in this example is to write a proper implementation that faithfully adheres to the original requirements.  Once the proper code has been written, a "success" result should be obtained by re-running the test.  After completing this final step, both the implmentation and the test are saved as important pieces of the application's codebase.  

Henceforth, any time the codebase undergoes new revisions, all the tests in the codebase are re-run.  In this way, a team gains confidence that working features are not accidentally broken by future changes.

Depending on the schema definition of the Customer table, the query shown in Figure {} may or may not be a correct implementation of the "Orders Per Customer" report.  If the Customer.email column is declared with a NOT NULL UNIQUE constraint, then indeed the query is correct.  (Recall, also, the simplifying assumption that each customer is guaranteed to have one or more orders.)

The fact that a query's correctness can depend on the underlying database schema raises two key points.  One is that the automated verification system will need to have access to the schema definitions.  The other point is that in the running TDD example, the schema definitions must also be tracked as essential parts of the codebase alongside the query code and the tests.

So far in this discussion of TDD, one could cynically conclude that TDD simply adds extra effort while still ultimately producing the same query that could have been written without TDD.  The payoff for the extra effort is about to become clear.

Assume that Customer.email is indeed constrained as NOT NULL UNIQUE.  Further assume that TDD was followed and that the SQL implementation of Figure {} passed the test.  The test, the query, and the schema definitions became part of the codebase.

Now imagine that a year has gone by, and the development team hardly remembers implementing the Orders Per Customer report.  Perhaps the developer who authored the query is no longer even employed at the company.  Now the development team is working on a new change to the system that involes altering the definition of the Customer table.  Management now requires that two or more customer id numbers be allowed to use the same email address.  The Customer.email column will no longer be marked as UNIQUE.  Without testing, this new situation would lead to errors such as the one shown in Figure {}.  The report in Figure {} indicates that there is one customer with the email address "c@abc.com" that is responsible for three orders.  The report is flawed.

The user of tool-supported TDD as described in this section can prevent application bugs like the one in Figure {}.  In the running example, the chosen test was [ assertion ].  The schema definitions are part of the codebase, and whever the codebase changes, the tests are re-run.  When the Customer schema is changed to remove UNIQUE from the email column, the TDD test designed for the Orders Per Customer report will fail.  The failed test will alert the developers so that they avoid accidentally breaking the report.

The ability to prevent flaws like that of Figure {} is the single most important benefit offered by a SQL verification tool.  Without a tool that considers the user's expectations about a query's outcome, the bug shown in Figure {} is virtually impossible to detect.  The code in Figure {} is syntactically correct and runs without any runtime error.

Therefore, without automated test support to detect the undesirable behavior change, the Orders Per Customer report would *silently* *break* following a DDL revision to remove UNIQUE from the email column.

The phrase *silently break* is used to indicate that the report executes without any hint of a problem, yet its output may contain erroneous data.  Worst of all, any erroneous data will be impossible to detect when viewing only the report output in isolation.

The prospect of introducting these silent flaws into an application is frightful to many developers.  For many development teams, the preferred approach for addressing the issue is simply to avoid changing the database at all costs [?? refactoring?].  Avoidance of schema changes leads to its own set of problems, as developers must cope with a database that becomes less and less suited to support the evolving application over time [?].

There is great value in any tool that can empower teams to update their database schemas without the fear of silent breakages.  This is a key motivating factor of this thesis.
