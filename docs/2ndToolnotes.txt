

TALK ABOUT HOW WHEN THE FORMULA IS INVALID, WE WOULD LIKE TO SEE A COUNTEREXAMPLE.


'complete set'. propositional formula using only /\ and NOT. the other connectives can be expressed in terms of those two.

'complete set' for relational algebra: select, project, union, minus, cartesian product.  a join can be expressed as a cartesian product followed by a select. intersection can be expressed using union and minus.



We need to show that relational SQL statements convert readily into FOL statements.

Relational calculus is...

Therefore, relational calculus will contain the usual FOL symbols /\, ->, and so forth, combined with various predicate symbols.  The relational calculus does not directly use (JOIN, UNION) ...   In some ways, the relational calculus is 'harder to read' (yuck. fix this) ... It might not be immediately obvious what the predicate symbols 'map to' in SQL ... (ok, that is a bit better than 'hard to read') ...   (still fix this).  Therefore, it will be helpful to breifly touch upon the well-known operations of relational alebra.  These operations can all be expressed in relational calculus.  However, they are more familiar to many SQL students as being operations of the relational algebra.   (WHOA.. that whole paragraph needs help. but i need to digress into algebra, and have to introduce it somehow).

Many of the most common SQL keywords are essentially "wrappers" or "syntactic sugar" for relational algebra formula.  This is true for keywords like JOIN, UNION, WHERE, and SELECT.  The SELECT keyword, however, does not correspond to the relational algebra notion of selection, but rather to the relational operation known as "project."

Many discussions of the relational model for database theory introduce relational algebra first, and then discuss relational calculus in turn.  Relational algebra and relational calculus have been shown to be equivalent.

Relational calculus is...

There are two methods of constructing an FOL language of relational calculus.  One is known as the tuple relational calculus, and the other is known as the domain relational calculus.  The Prolog expressions that will be developed in this chapter are based most closely on the domain relational calculus.  Basic variables in the domain relational calculus are placeholders for values from the domains of the attributes in the database.  In other words, (to use an informal description that is closer to day-to-day SQL parlance), each variable holds a value that is permissible in a "column" of some table.  If the database has a table that contains a column (or columns) of type integer, then the corresponding domain relational calculus expressions will use variables that range over the integers.  Sets of domain variables can be strung together in a sequence to build a tuple.  Following this domain relational approach, our Prolog code first defines our domains and domain variables, and then builds tuple structures from there.  This differs from tuple relational calculus, where the fundamental variables are tuples, and the tuple components are accessed via numeric or named indices applied to the tuple.

The tuple relational calculus and the domain relational calculus are equivalent, and they are both equivalent to the relational algebra.  (i kind of said this earlier... uh-oh, cleanup time again).  However, these calculi and the equivalent algebra are not sufficient to express aggregation.  In this chapter, we also devise logical formulas to describe aggregation.  Research by several other teams has outlined similar ways of augmenting the original calculus with additional axiomatized operations in order to formally express features in SQL that do not exist in the original relational calculi.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
(SECTION: domains, tuple types, and table types ...   in the system...)

Using the domain relational calculus as a guide, the first definitions (objects? statements?) we create in (POQ?) are ... (ugh... phrase not happening for me right now)

For example, the following Prolog statements are used to define a finite domain 'natural_type,' which is a subset of the natural numbers.  Later we will use this domain to build tuples where one or more attributes of the tuple are of type natural_type.

natural_type(0).
natural_type(1).
natural_type(2).
natural_type(3).
natural_type(4).

In Prolog, we can then specify a domain variable to range over natural_type by writing:

natural_type(X).

Any symbol beginning with a capital letter is always a variable in Prolog.  This capitalization rule is what allows us to unambiguously detect that 'X' is a variable in the preceding Prolog statement.  However, it is also what forces us to use lower-case names for some of the items that follow, such as 'null', even though in SQL we typically see this concept represented as 'NULL' and not 'null'.

Natural numbers are used in databases in many ways.  A column that represents, say, the number of cartons of roofing shingles in stock at a warehouse would likely use the natural number type.  Natural numbers are also often used as unique row identifier numbers in database tables.  In additional to natural numbers, another datatype that is used to uniquely label rows is the Globally Unique Identifier type, or GUID. We add this domain to our system as follows:

guid_type(fccy463).
guid_type(srce544).
guid_type(ddd213).
guid_type(tchc397).

Actual GUID values are longer than 'fccy463' and the other values shown above.  The choice of using shortened values to represent GUIDs is a deliberate design decision in our system.  To explain this decision, it is helpful to first consider the role that these values will play in the system.  To restate a prior point, the values that we use to populate the finite domain of each type are precisely the values that will later appear as attributes in concrete tuples.  However, in cases where assertions are proved valid, the end user of the proof system may never need to view any of the contrete tuples that were created during the proof process.  So in the case of a successful proof, it does not matter too much what our values look like, as long as they map closely enough to a real-life representation of the database so that we can have confidence in the proof.  In cases where assumptions are invalid, then our design requirements for these values become a bit more stringent.  As stated earlier, the detection of invalid statements will cause the system to output a counterexample.  The counterexample is an assignment of values to each variable that was featured in the assertion.  Recall that the variables in the assertion can represent table values, row values, and parameter values (in cases where the SQL statement is parameterized).  This means that the system's GUID values--such as 'fccy463'--will be shown to the user as part of a concrete counterexample.

Depending on whether we expect our end user to have a flexible imagination or not, we might accept or reject certain design criteria in defining our GUID domain.  The design criteria that have been applied here are simple.  When a GUID is printed out as part of a counterexample, our user must recognize this value as representing a plausible potential value that might someday get inserted into the pertinent database table.  The value does not need to be an actual value that is currently present in the real-life database.  Quite the contrary, because if it did need to be present currently, then our tool would only check the satisfiability or unsatisfiability of a SQL statement given the current database state, and that is not the intent of the tool at all.  The intent of the tool is to prove that certain properties of the SQL statement and its result set will hold true for all states that this database can attain, including past states, present states, and future states.  The user is expected to understand that 'fccy463' represents something that might someday appear in the database.  Even when the user knows that the appearance of 'fccy463' in a counterexample in no way indicates that 'fccy463' is present in the real-life database currently, the user could still raise objections.  One such objection would be that 'fccy463' is not formatted appropriately the way that real-life GUIDs are.  This is where the expectation of a "flexible imagination" comes into play.  It seems reasonable to expect that most trained users could easily know to "mentally map" a symbol like 'fccy463' into a realistic GUID such as '{5224F545-A443-4859-BA23-7B5A95BDC8EF}' in order to visualize the real-life equivalent of the counterexample, and in order to generate a real-life SQL script filled with test data for testing the counterexample, if needed.  The idea that our users will be flexible enough to accept "reasonably recognizable" values allows us to keep the Prolog code simple, and allows us to avoid any potential hassles in cases where a true-to-life representation of a datatype might involve the use of characters or strings that are difficult to work with in Prolog.

If there are any enum types in the database, then we can easily define such a type in our Prolog system:

color_enum_type(red).
color_enum_type(orange).
color_enum_type(yellow).
color_enum_type(green).

The most important type remaining is the string type.  The introduction of that type will involve another discussion of design trade-offs.  Therefore, before we consider strings, we shall cover the simple technique of supporting nullable attributes in our system.  We only need one way to represent NULL, so we will use the Prolog constant 'null.'  Constants must begin with lower-case letters in Prolog, so we are unable to use a constant that would faithfully coincide with the capitalized keyword that SQL uses.  Here we declare our constant as the only thing that is null:

isnull(null).

Then, we must add clauses that will allow 'null' to appear any place where a number, GUID, or enum value can appear.  This is done as follows:

natural_type(null).
guid_type(null).
color_enum_type(null).

Ignoring the special case of a primary key column, the default behavior in SQL table creation (double-check standard. cite section of standard?) is to create each table column such that NULL can appear in any column, no matter the datatype of the column.  This observation is what motivated the inclusion of our 'null' constant into each domain of our system.  However, it is also possible to override the default nullability in SQL and declare columns that do not permit NULL.  To accomodate this possibility, we add the following shorthand:

not_null(X) :- \+isnull(X).

Instead of using the above declaration of notnull, we could simply type '\+isnull' wherever a predicate like notnull would appear.  However, the shorthand is easier on the eyes, and it more closely resembles the SQL syntax that it represents (i.e. 'NOT NULL').

So in order to add a string domain to the system, we know that we will add something like 'string_type(null)' in order to support nullable string columns.  After that, however, the possibilities are vast regarding what other items should become members of this finite domain.  In the earlier discussion of GUIDs, it was deemed reasonable to expect that a user would accept 'fccy463' as representing a GUID, even if it is an imperfect representative.  However, while we could also request that our users accept things like 'ejhrbxjs' or 'abcdef' as reasonable product names or employee names, at best this seems like a significant distraction.  At worst, it could be a significant obstacle to the user's ability to comprehend some counterexample assignments produced by the system.

Therefore, for usability reasons, the system will actually support multiple string domains such as name_string_type and product_string_type.  This way, a counterexample involving a Product table would display values like "desk," "book," "bottle," or "car" instead of gibberish strings.  To pay the cost of this usability feature, however, a burden of extra annotations must be imposed.  When the system parses SQL table definitions, it will no longer be sufficient for the system to recognize a SQL type declaration as "some string type."  Instead, a specific annotation should be provided so that the proof system can be alerted to map the column's type specifically to name_string_type, product_string_type, or some other specific subset of string representatives.

Now that some basic types have been defined, it will be possible to define tuples composed of these types.  While it was easy to list some helpful datatypes without considering any particular database, it is not possible to populate our system with tuple types in the same (free-form? ad-hoc? open-ended?) way.  The set of datatypes available in SQL databases are fairly uniform across the different database products.  Therefore, other than the idea of tailoring our string types to look sensible in particular scenarios, we can prepopulate our system with several finite domains that will likely be useful for proofs involving any database we encounter.  The same cannot be said for tuple types.  The tuple types that we will require for proofs about a given database are likely to be unique to that particular database.  Therefore, concrete tuple types are something that the proof system will have to generate after parsing the 'CREATE TABLE' statements that define the given database.

To give an example of a tuple type specification in the proof system, we need a concrete table defintion as input.  Consider the following table defition:

CREATE TABLE Product ( product_id int, product_name varchar(100), PRIMARY KEY (product_id) );

From the table definition, it can be determined that the tuple will have two components: one numeric component and one string component.  The string component will have the default nullability.  However, because the numeric component is also the table's primary key in this case, SQL will automatically disallow it from ever containing NULL.  Based on that, the system can create the following tuple type definition:

product_tuple( PROD_ID, PROD_NAME ) :-
    natural_type(PROD_ID), notnull(PROD_ID),
    product_string_type(PROD_NAME).

The above is a logical formula in Prolog that states that any sequence of variables ( PROD_ID, PROD_NAME ) is indeed a product_tuple as long as PROD_ID is a natural number value and not null, and PROD_NAME is a valid string.  In the case of PROD_NAME, since the nonnull predicate has not been asserted for that variable, we would also allow the sequence ( PROD_ID, null ) to be a product tuple, as long as the requirements on PROD_ID are met.

Of course, rather than asserting that "if requirements are met, then ( PROD_ID, PROD_NAME ) is a product tuple," we would really like use an "if and only if" (bi-implication) instead of a simple "if" (implication).  Prolog does not have a symbol for logical bi-implication.  However, if no other implication is provided that can imply that ( PROD_ID, PROD_NAME ), then we have essentially achieved bi-implication due to the behavior of Prolog.  Prolog will only ever allow the the product_tuple conclusion to be true when Prolog can satisfy the body of a concrete clause where the head symbolizes the satisfaction of the product_tuple predicate.  The product_tuple clause shown above would be the only one our system would generate with product_tuple as the head, so this would be the only way allowable to conclude that some sequence ( PROD_ID, PROD_NAME ) is a permissible product tuple.

Now that tuples have a place in the system, the only remaining structure to define is the table types that are formed from collections of tuples.  A SQL table is an approximation of a relation.  A relation in a strict sense is an unordered set of tuples.  Note that the definition of set means that a relation will never contain duplicate tuples.  In a SQL database, on the other hand, it is possible to create tables that contain duplicate rows.  Additionally, a SQL database usually only provides a very imperfect sense of the supposed lack of ordering over rows.  The database will typically display a result set in the same order no matter how times a query is run, and many times the resulting rows will appear to have been sorted by some numeric column, even when the query did not include any keywords that would require an order.

To maximize the kinds of SQL query bugs that our system can catch, we need to emulate the non-set characteristic of SQL tables that admits duplicate rows in a table.  Otherwise, if our system obeyed a faithful set-based description of relations, then the proof system might uphold a user's assertion about the uniqueness of some row, only to be disappointed when contrary results are observed in the real-life database.  Perhaps counter-intuitively, a very different rationale will apply to making design decisions about whether to emulate SQL's tendency to obey some apparent ordering when no ordering is required.  In the case of SQL's "apparent" result ordering, we would provide a more valuable service by deliberating seeking to avoid emulating this behavior.  

The argument in favor of avoiding emulation of apparent ordering is based on the observation that such ordering can change on a moment's notice, sometimes after being stable for months or years.  This almost always causes unfortunate side-effects whenever a SQL user had been lulled into relying on the ordering.  When inherently unordered queries produce "apparently ordered" results, this is usually because the internal algorithms of the RDB store and retrieve data relying on fast-lookup indices.  If a database administrator decides to drop an index or reformulate an index, then the RDB might suddenly be caused to answer a long-standing query using a different retrieval order than before.  This is virtually certain to cause application errors if the application had been designed to attach some meaning or ranking based on the order of the output rows from the query.  The SQLite RDB contains an interesting feature to help users detect such errors.  The SQLite command 'PRAGMA reverse_unordered_selects;' affects queries that do not contain any special keywords that require the sorting of results.  When the pragma is enabled, the RDB will output result sets for such queries in the *reverse* order of whatever the retrieval algorithm normally would have done.  By alternating whether this pragma is enabled or disabled, SQLite users have a better chance of noticing where faulty presumptions of sorting have led to bugs in their application.

The preceding discussion establishes the relevant properties that our table types should possess.  Designing a structure that admits duplicate tuples is not difficult.  However, attempting to provide a true sense of the absence of any ordering is potentially trickier.  A list structure obvious has an order, so using the built-in Prolog list type to express SQL tables would at first glance seem problematic.  Nevertheless, Prolog lists are the chosen method in our system.  Thankfully, it turns out that the nature of Prolog's execution engine neutralize any concerns over undesirable ordering.  In our Prolog-based proof system, the backtracking behavior of Prolog's proof search will be sufficient to ensure avoidance of any "apparent" (yet unguaranteed) ordering.  Concretely, if an assertion needs to be valid for a set of product tuples regardless of the order in which they appear, Prolog will eventually test many possible reorderings.






