

In chapter UUU, a number of common SQL pitfalls were listed and described.  Most experienced SQL practicioners are aware of these pitfalls.  Being aware of the potential errors, however, is not enough to make a person immune to committing them in the future.  Despite not being a cure-all, knowledge of potential trouble-spots is undeniably helpful during coding.  Once coding is finished, knowledge of trouble-spots can have an even bigger payoff during code reviews, testing, and debugging.  It is rare for a software project to achieve full test coverage; most project teams do not even attempt it.  In light of this, heuristics such as lists of common pitfalls become valuable guideposts so that testing efforts are focused on the pieces of code and the combinations of inputs that are most likely to uncover problems. (whether the 'list of pitfalls' explicitly documented or subconsciouly internalized by experienced teams) .

Imagine that we have a body of relational SQL code, we have an awareness of several pitfalls the code may have failed to correctly avoid, and we are now tasked with testing whether the code steers clear of the pitfalls or not.  What options are available for performing such testing?  Manual inspection is one possibility.  This is time consuming and error prone.  Running the code against one or more populated databases is another option.  To do so, some technique must be devised for populating the test database.  The database must also be dropped and regenerated and populated in several different ways in order to check that the SQL code is well-behaved in a variety of possible settings.  Tools exist for generating test data to populate test databases.  In general, however, such data generation tools are not "query aware."  This means that test data is populated into tables without regard to how the tables are actually queried.  Because of that, a table may be dropped and repopulated many times without ever creating a query-specific condition that would reveal a bug in a particular query.  In the absense of an off-the-shelf query-aware data generator, some ad-hoc script could be written to fulfill the same goal.  For example, if we have reason to doubt that a query will yield the desired outcome specifically when one given table is empty and another three tables are non-empty, then we must write a script to generate a populated test database meeting that particular criterion.

None of the testing options just described is ideal.  Given a SQL query and a suspected weakness of the query, it would be ideal to have a simple and succint way to input both the query and the suspected weakness into an automated system, and quickly obtain a yes/no answer as to whether the suspected flaw is truly present or not.  In other words, the query and the flaw are both known, so the best tool possible would be a tool that takes these two known pieces and automatically outputs "confirmed flawed" or "confirmed free of targeted flaw."

The main argument of this thesis is that such a tool is possible.  In this chapter a thorough high-level design for such a tool is proposed.  Rather than repeatedly employing phrases such as "the proposed tool" throughout this chapter, the tool is given the name POQ.  POQ is an acronym for Postconditions On Queries.  (The intended pronunciation is identical to the word "poke.")  An automated implementation of POQ has not yet been completed.  Its intended future architecture will be outlined here.  Also, the operations that POQ will one day perform automatically have been performed manually and will be described in detail.  Despite the fact that POQ is currently only a hypothetical tool, giving the tool a proper name is merited for several reasons.  One (as stated earlier), awkward noun phrases -- such as "some future tool like the one proposed here" -- can be avoided whenever POQ is the subject of discussion.  Two, the author intends to pursue development of a full-featured implementation of POQ.  Therefore, using the tool's proper name in this paper will ensure that the identify of the future POQ tool is clearly linked back to this thesis.  Three, focusing discussion around the name of the ongoing POQ development project emphasizes that this research is heavily oriented towards producing practical outcomes with concrete applicability in the present day.

As stated previously, POQ's role is to process two inputs and produce a pass/fail result.  The two inputs were referred to as SQL code and a suspected flaw.  By its inherent nature, SQL code is already formatted suitably to be considered machine-readable input.  Therefore, to design the aforementioned ideal tool, the ambiguous piece of the design is that a format must be found for encoding a suspected flaw.  The format to be used in this chapter will be FOL formulas expressed in Prolog syntax.

The previous discussion was couched in terms of software testing.  However, it was made evident earlier that this thesis deals principally with verification.  Hopefully it is becoming apparent now that in the realm of relational SQL code, verification and testing are not too far separated from one another.  Relational SQL is declarative.  It is difficult to think of what it would mean to "test a declaration" without relying on formal analysis in one form or another.  If a SQL query is taken as a logical formula, then the natural tests to perform are satisfiability and validity tests.  Representing a "suspected flaw" using FOL formulas is really the act of creating a formally-specified postcondition.  The only difference is whether negation is applied.

For example, a suspected flaw might involve ____.  In FOL, we need to represent " a ___  ____."  One way would look like this:  .  Taking the point of view of formal verification, the corresponding postcondition would express that " a ___ *never* ____s."  The poscondition specifies the correct behavior of the SQL code.  The encoding of the flaw describes a corresponding incorrect behavior.  One is the negation of the other.

----------

The pitfalls described in chapter UUU were all demonstrated using (on average) only about two base tables.  Admittedly, when only two tables participate in a query, manual inspection can indeed be an effective and efficient method for error-checking.  In this chapter, as well, examples are presented using as few tables and as few columns as possible.

relational queries can be much larger (in practice)
tables can be of much greater arity (in practice)
DDL is not always kept close at hand while coding (in practice)
SQL operators often imply an on-the-fly generation of numerous 'virtual tables' while generating the final resultset.

----------

Figure JJJ illustrates several fundamental points about the high-level architecture of POQ and a sort of high-level 'call graph' showing how various modules call each other inside the system.

The details of the UI Layer are intentionally left open/unspecified/vague in this thesis.  For an evaluation of whether flaw-detection in SQL code can be automated, it is immaterial whether the automatic tool has a windowed GUI interface, a command-line interface, or a web- or tablet-based interface.  The discussion of POQ only requires that some interface be assumed, so that the tool has a way to receive the SQL code and postcondition assertions as input.

The processing flow depicted in Figure JJJ is intended to represent the processing of a single query.  A single query is defined as one SELECT statement terminated with a semicolon.  The SELECT statement may be of an arbitrary length and composition, as long as it is syntactically correct SQL code.  All the examples in this chapter are aligned with the idea of processing a single query at a time.

Even for processing just one query, the system requires a more SQL code as input than just the single statement beginning with SELECT and ending with a semicolon.  The SQL code that must be inputted for one round of processing includes the query *and* all CREATE TABLE statements that define each table used in the query.  Both inputs are required.  POQ does not operate on SELECT statements that reference zero tables; therefore, at least one CREATE TABLE declaration will always be required.  Wherever this chapter mentions "inputting SQL code" into the tool, the intent is to refer to a single query accompanied by all relevant CREATE TABLE statements.  This is also evident in each example.

Just as box 1a in Figure JJJ signifies input of both a SQL query and SQL table declarations, box 1b also represents the inputting of two types of assertion annotations.  The assertions may include both a precondition formula and a postcondition formula.  Only the postcondition is required.  Postconditions represent a contract or guarantee that the SQL query will never contain some specific manner of flawed output.  Preconditions, on the other hand, place restrictions on when and where the query can safely be called.  Whenever code is intended to be general-purpose and highly reusable--be it SQL code, C code, Java code, or other--it is common to avoid placing preconditions into the code contract.  By using the "empty precondition" (more formally known as the trivial precondition >>> T <<<), the code's author indicates that the code may be called at any time and it will adhere to its postconditions in every case.  In other words, the abscense of a precondition indicates highly reusable code--a good thing.  The abscense of a postcondition, on the other hand, is generally bad; it indicates an abscense of any guarantee as to whether the code is well-behaved.

Once the inputs are received, execution transfers to the middle layer (Synthesis & Translation).  This middle layer performs many important steps in preparation for sending a proof task to the prover engine.  The SQL code must be parsed and found to be syntactically correct.  Each base table named in the SQL query must properly resolve to some table described in the inputted CREATE TABLE statements.  Basic parsing and syntax-checking should also be performed on the annotations.  Based on the results of parsing, the Synthesis & Translation Layer subsequently proceeds to generate FOL axioms and proof goals.

The steps for transforming parsed SQL code into axioms will be showcased throughout this chapter.  Parsing and transformation of assertions, on the other hand, is ommitted from discussion.  This omission is possible due to a simplifying assumption mentioned earlier.  The assumption is that the assertions will be inputted in the form of FOL formulas expressed in Prolog syntax.  This means that the input provided is already in the format that can be fed to the Prover Engine layer.  Just as one might suspect, this simplifying assumption does not match up with how most industrial verification frameworks operate.  (examples: frama-c, esc/java, eiffel)

In future versions of POQ, the assumption that assertions arrive into UI layer in a format already suitable for the Prover Engine is likely to be removed.  That will require the design of a mini-language expressly devised to encode the particular assertions supported by POQ.  There is currently no known prior tool for annotating relational SQL statements with postconditions.  Therefore, it is difficult to find inspiration as to what such a mini-language should look like.  Design of such a language is left for future work.  Nonetheless, the Prolog-syntax assertion notation used in this thesis is arguably a decent starting point.  Even though the motivation for providing the assertions as Prolog code was initially motivated only by convenience (since Prolog is the language of the Prover Engine), these assertions turned out to be surprisingly intuitive, easy to read, and easy to write.  Here (an excerpt... or not) is one such assertion:

    member(),
    member()

Given how straightforward it is to create assertions like the one show above, it does not seem out of the question that even a first public release of POQ might require users to input their assertions in such a syntax.  Even if this syntax does persist in future versions of POQ, the tool will still need to perform a kind of type-checking validation on these assertions.  For example, an assertion containing "~~~~~~~~~~~~~~" should only be accepted if the table ~~~~ is composed of exactly %%%555 columns.  If instead the table only consists of %%%444 columns, then such a statement will always be trivially unsatisfiable, because table ~~~~ can never contain *any* tuple of arity %%%555.  Badly formulated assertions like these must be flagged before the prover engine is invoked.  Otherwise, the results from the prover engine would be garbage, and it would would be a grave disservice to present the garbage proof results to the user.


After parsing the inputted SQL and the inputted assertions, the Synthesis & Translation layer will create logical axioms that faithfully describe the behavior of the given SQL query and the constraints upon the contents of the relevant SQL tables.  These axioms will all be passed as input to the Prover Engine.  Therefore, the axioms must be encoded in a language understood by the Prover Engine.  In the present case, this language is Prolog.  Using the assertions, a single proof goal will also be formulated.  The proof goal is built using both the axioms and the assertions.  As discussed earlier, if a postcondition asserts a formula >>phi>>, the proof goal will actually be formulated to try and satisfy >>NOT-phi>>.  If a satisfying assignment is found for >>NOT-phi>>, then we call this assignment a counterexample.  The counterexample demonstrates that >>phi>> cannot be guaranteed by the current SQL query.  In prose, the proof goal intuitively represents the following question:  in any model database that obeys the axioms of this SQL scenario, will the SQL resultset from this query ever satisfy >>NOT-phi>>?  Again, >>phi>> would represent a postcondition (a guarantee), and >>NOT-phi>> can be seen as a description of a flaw.  Finding a satisfying assignment for >>NOT-phi>> amounts to finding incontrovertible evidence of a flaw.  Failing to satisfy >>NOT-phi>>, on the other hand, means the query is guaranteed to always obey the postcondition.  This is why a result of "false" from the Prolog Prover Engine is indeed a happy outcome indicating proper behavior of the SQL code with respect to its specification.

Step 3 in Figure JJJ is accomplished simply by loading the axioms and goals into SWIPL Prolog and having Prolog resolve the goal.  Because of the negated approach, when Prolog outputs "false" this actually indicates to POQ that the desired properties of the query are valid.  In prose, this sounds backwards and counter-intuitive.  However, it will become very clear in the examples given later in the chapter.  To review the details regarding how Prolog makes a true/false judgement of a goal, refer to Chapter OOO.

Once POQ completes Step 3 from Figure JJJ, information begins flowing upward through the layers until it arrives at the UI for viewing by the user.  In case of a successful proof, the Prolog Prover Engine simply outputs "false," as explained in the preceding paragraph.  When that happens, it is trivial to make the Synthesis & Translation layer create a translation of the result into a more user-friendly message such as:  "the query is correct with regard to the given specification."  The more interesting case is when a counter-example is found that invalidates the specification.  The Prolog engine produces counter-examples that look like this:

    TABLE_X = [(a,b,22),(a,null,33)],
    TABLE_Y = [(a,null)]

There are at least two helpful transformations that the Synthesis & Translation layer could apply to such a result.  One is to display essentially the same information as shown above in Prolog, but to render it in a tabular format that is familiar to SQL users.  Another is to generate a complete SQL script that would create and populate SQL tables, run the original SQL query, and demonstrate the entire counter-scenario inside an actual RDB system (and outside of POQ).  Users could execute such scripts in an RDB system of their choosing.  Both of these potential transformations are shown in Figure LLL.  Again, the UI Layer is left unspecified in this thesis.  Therefore, no choice is made here as to whether the tabular counter-example and/or the counter-example SQL script would be displayed in a graphical window or printed at the command-line, or displayed or transmitted in any other manner.


??????????????????
??????????????????
    conclude this section. ui layer not treated. prover engine is SWIPL prolog. therefore, focus is now entirely on Synthesis & Translation layer.  According to Figure JJJ, the duties of ___ layer are 2a, 2b, and 4.  Step 4 has been covered sufficently.  Step 2b is ommitted due to the simplifying assumption.  Step 2a is the focus of the rest of this chapter.
??????????????????
??????????????????

=========================================================

