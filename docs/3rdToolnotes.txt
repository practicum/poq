
we would have described the 'frama-c' WHY3 type of imperative proving.

we claimed that 'weakest precondition' steps are not needed, since 'SQL is FOL'. explained we base things on DOMAIN relational calculus.



We need to show that relational SQL statements convert readily into FOL statements.

Relational calculus is...

Therefore, relational calculus will contain the usual FOL symbols /\, ->, and so forth, combined with various predicate symbols.  The relational calculus does not directly use (JOIN, UNION) ...   In some ways, the relational calculus is 'harder to read' (yuck. fix this) ... It might not be immediately obvious what the predicate symbols 'map to' in SQL ... (ok, that is a bit better than 'hard to read') ...   (still fix this).  Therefore, it will be helpful to breifly touch upon the well-known operations of relational alebra.  These operations can all be expressed in relational calculus.  However, they are more familiar to many SQL students as being operations of the relational algebra.   (WHOA.. that whole paragraph needs help. but i need to digress into algebra, and have to introduce it somehow).

Many of the most common SQL keywords are essentially "wrappers" or "syntactic sugar" for relational algebra formula.  This is true for keywords like JOIN, UNION, WHERE, and SELECT.  The SELECT keyword, however, does not correspond to the relational algebra notion of selection, but rather to the relational operation known as "project."

Many discussions of the relational model for database theory introduce relational algebra first, and then discuss relational calculus in turn.  Relational algebra and relational calculus have been shown to be equivalent.

Relational calculus is...

There are two methods of constructing an FOL language of relational calculus.  One is known as the tuple relational calculus, and the other is known as the domain relational calculus.  The Prolog expressions that will be developed in this chapter are based most closely on the domain relational calculus.  Basic variables in the domain relational calculus are placeholders for values from the domains of the attributes in the database.  In other words, (to use an informal description that is closer to day-to-day SQL parlance), each variable holds a value that is permissible in a "column" of some table.  If the database has a table that contains a column (or columns) of type integer, then the corresponding domain relational calculus expressions will use variables that range over the integers.  Sets of domain variables can be strung together in a sequence to build a tuple.  Following this domain relational approach, our Prolog code first defines our domains and domain variables, and then builds tuple structures from there.  This differs from tuple relational calculus, where the fundamental variables are tuples, and the tuple components are accessed via numeric or named indices applied to the tuple.

The tuple relational calculus and the domain relational calculus are equivalent, and they are both equivalent to the relational algebra.  (i kind of said this earlier... uh-oh, cleanup time again).  However, these calculi and the equivalent algebra are not sufficient to express aggregation.  In this chapter, we also devise logical formulas to describe aggregation.  Research by several other teams has outlined similar ways of augmenting the original calculus with additional axiomatized operations in order to formally express features in SQL that do not exist in the original relational calculi.





In chapter UUU, a number of common SQL pitfalls were listed and described.  Most experienced SQL practicioners are aware of these pitfalls.  Being aware of the potential errors, however, is not enough to make a person immune to committing them in the future.  Despite not being a cure-all, knowledge of potential trouble-spots is undeniably helpful during coding.  Once coding is finished, knowledge of trouble-spots can have an even bigger payoff during code reviews, testing, and debugging.  It is rare for a software project to achieve full test coverage; most project teams do not even attempt it.  In light of this, heuristics such as lists of common pitfalls become valuable guideposts so that testing efforts are focused on the pieces of code and the combinations of inputs that are most likely to uncover problems. (whether the 'list of pitfalls' explicitly documented or subconsciouly internalized by experienced teams) .

Imagine that we have a body of relational SQL code, we have an awareness of several pitfalls the code may have failed to correctly avoid, and we are now tasked with testing whether the code steers clear of the pitfalls or not.  What options are available for performing such testing?  Manual inspection is one possibility.  This is time consuming and error prone.  Running the code against one or more populated databases is another option.  To do so, some technique must be devised for populating the test database.  The database must also be dropped and regenerated and populated in several different ways in order to check that the SQL code is well-behaved in a variety of possible settings.  Tools exist for generating test data to populate test databases.  In general, however, such data generation tools are not "query aware."  This means that test data is populated into tables without regard to how the tables are actually queried.  Because of that, a table may be dropped and repopulated many times without ever creating a query-specific condition that would reveal a bug in a particular query.  In the absense of an off-the-shelf query-aware data generator, some ad-hoc script could be written to fulfill the same goal.  For example, if we have reason to doubt that a query will yield the desired outcome specifically when one given table is empty and another three tables are non-empty, then we must write a script to generate a populated test database meeting that particular criterion.

None of the testing options just described is ideal.  Given a SQL query and a suspected weakness of the query, it would be ideal to have a simple and succint way to input both the query and the suspected weakness into an automated system, and quickly obtain a yes/no answer as to whether the suspected flaw is truly present or not.  In other words, the query and the flaw are both known, so the best tool possible would be a tool that takes these two known pieces and automatically outputs "confirmed flawed" or "confirmed free of targeted flaw."

The main argument of this thesis is that such a tool is possible.  In this chapter a thorough high-level design for such a tool is proposed.  Rather than repeatedly employing phrases such as "the proposed tool" throughout this chapter, the tool is given the name POQ.  POQ is an acronym for Postconditions On Queries.  (The intended pronunciation is identical to the word "poke.")  An automated implementation of POQ has not yet been completed.  Its intended future architecture will be outlined here.  Also, the operations that POQ will one day perform automatically have been performed manually and will be described in detail.  Despite the fact that POQ is currently only a hypothetical tool, giving the tool a proper name is merited for several reasons.  One (as stated earlier), awkward noun phrases -- such as "some future tool like the one proposed here" -- can be avoided whenever POQ is the subject of discussion.  Two, the author intends to pursue development of a full-featured implementation of POQ.  Therefore, using the tool's proper name in this paper will ensure that the identify of the future POQ tool is clearly linked back to this thesis.  Three, focusing discussion around the name of the ongoing POQ development project emphasizes that this research is heavily oriented towards producing practical outcomes with concrete applicability in the present day.

As stated previously, POQ's role is to process two inputs and produce a pass/fail result.  The two inputs were referred to as SQL code and a suspected flaw.  By its inherent nature, SQL code is already formatted suitably to be considered machine-readable input.  Therefore, to design the aforementioned ideal tool, the ambiguous piece of the design is that a format must be found for encoding a suspected flaw.  The format to be used in this chapter will be FOL formulas expressed in Prolog syntax.

The previous discussion was couched in terms of software testing.  However, it was made evident earlier that this thesis deals principally with verification.  Hopefully it is becoming apparent now that in the realm of relational SQL code, verification and testing are not too far separated from one another.  Relational SQL is declarative.  It is difficult to think of what it would mean to "test a declaration" without relying on formal analysis in one form or another.  If a SQL query is taken as a logical formula, then the natural tests to perform are satisfiability and validity tests.  Representing a "suspected flaw" using FOL formulas is really the act of creating a formally-specified postcondition.  The only difference is whether negation is applied.

For example, a suspected flaw might involve ____.  In FOL, we need to represent " a ___  ____."  One way would look like this:  .  Taking the point of view of formal verification, the corresponding postcondition would express that " a ___ *never* ____s."  The poscondition specifies the correct behavior of the SQL code.  The encoding of the flaw describes a corresponding incorrect behavior.  One is the negation of the other.

----------

The pitfalls described in chapter UUU were all demonstrated using (on average) only about two base tables.  Admittedly, when only two tables participate in a query, manual inspection can indeed be an effective and efficient method for error-checking.  In this chapter, as well, examples are presented using as few tables and as few columns as possible.

relational queries can be much larger (in practice)
tables can be of much greater arity (in practice)
DDL is not always kept close at hand while coding (in practice)
SQL operators often imply an on-the-fly generation of numerous 'virtual tables' while generating the final resultset.

----------

Figure JJJ illustrates several fundamental points about the high-level architecture of POQ and a sort of high-level 'call graph' showing how various modules call each other inside the system.

The details of the UI Layer are intentionally left open/unspecified/vague in this thesis.  For an evaluation of whether flaw-detection in SQL code can be automated, it is immaterial whether the automatic tool has a windowed GUI interface, a command-line interface, or a web- or tablet-based interface.  The discussion of POQ only requires that some interface be assumed, so that the tool has a way to receive the SQL code and postcondition assertions as input.

The processing flow depicted in Figure JJJ is intended to represent the processing of a single query.  A single query is defined as one SELECT statement terminated with a semicolon.  The SELECT statement may be of an arbitrary length and composition, as long as it is syntactically correct SQL code.  All the examples in this chapter are aligned with the idea of processing a single query at a time.

Even for processing just one query, the system requires a more SQL code as input than just the single statement beginning with SELECT and ending with a semicolon.  The SQL code that must be inputted for one round of processing includes the query *and* all CREATE TABLE statements that define each table used in the query.  Both inputs are required.  POQ does not operate on SELECT statements that reference zero tables; therefore, at least one CREATE TABLE declaration will always be required.  Wherever this chapter mentions "inputting SQL code" into the tool, the intent is to refer to a single query accompanied by all relevant CREATE TABLE statements.  This is also evident in each example.

Just as box 1a in Figure JJJ signifies input of both a SQL query and SQL table declarations, box 1b also represents the inputting of two types of assertion annotations.  The assertions may include both a precondition formula and a postcondition formula.  Only the postcondition is required.  Postconditions represent a contract or guarantee that the SQL query will never contain some specific manner of flawed output.  Preconditions, on the other hand, place restrictions on when and where the query can safely be called.  Whenever code is intended to be general-purpose and highly reusable--be it SQL code, C code, Java code, or other--it is common to avoid placing preconditions into the code contract.  By using the "empty precondition" (more formally known as the trivial precondition >>> T <<<), the code's author indicates that the code may be called at any time and it will adhere to its postconditions in every case.  In other words, the abscense of a precondition indicates highly reusable code--a good thing.  The abscense of a postcondition, on the other hand, is generally bad; it indicates an abscense of any guarantee as to whether the code is well-behaved.

Once the inputs are received, execution transfers to the middle layer (Synthesis & Translation).  This middle layer performs many important steps in preparation for sending a proof task to the prover engine.  The SQL code must be parsed and found to be syntactically correct.  Each base table named in the SQL query must properly resolve to some table described in the inputted CREATE TABLE statements.  Basic parsing and syntax-checking should also be performed on the annotations.  Based on the results of parsing, the Synthesis & Translation Layer subsequently proceeds to generate FOL axioms and proof goals.

The steps for transforming parsed SQL code into axioms will be showcased throughout this chapter.  Parsing and transformation of assertions, on the other hand, is ommitted from discussion.  This omission is possible due to a simplifying assumption mentioned earlier.  The assumption is that the assertions will be inputted in the form of FOL formulas expressed in Prolog syntax.  This means that the input provided is already in the format that can be fed to the Prover Engine layer.  Just as one might suspect, this simplifying assumption does not match up with how most industrial verification frameworks operate.  (examples: frama-c, esc/java, eiffel)

In future versions of POQ, the assumption that assertions arrive into UI layer in a format already suitable for the Prover Engine is likely to be removed.  That will require the design of a mini-language expressly devised to encode the particular assertions supported by POQ.  There is currently no known prior tool for annotating relational SQL statements with postconditions.  Therefore, it is difficult to find inspiration as to what such a mini-language should look like.  Design of such a language is left for future work.  Nonetheless, the Prolog-syntax assertion notation used in this thesis is arguably a decent starting point.  Even though the motivation for providing the assertions as Prolog code was initially motivated only by convenience (since Prolog is the language of the Prover Engine), these assertions turned out to be surprisingly intuitive, easy to read, and easy to write.  Here (an excerpt... or not) is one such assertion:

    member(),
    member()

Given how straightforward it is to create assertions like the one show above, it does not seem out of the question that even a first public release of POQ might require users to input their assertions in such a syntax.  Even if this syntax does persist in future versions of POQ, the tool will still need to perform a kind of type-checking validation on these assertions.  For example, an assertion containing "~~~~~~~~~~~~~~" should only be accepted if the table ~~~~ is composed of exactly %%%555 columns.  If instead the table only consists of %%%444 columns, then such a statement will always be trivially unsatisfiable, because table ~~~~ can never contain *any* tuple of arity %%%555.  Badly formulated assertions like these must be flagged before the prover engine is invoked.  Otherwise, the results from the prover engine would be garbage, and it would would be a grave disservice to present the garbage proof results to the user.


After parsing the inputted SQL and the inputted assertions, the Synthesis & Translation layer will create logical axioms that faithfully describe the behavior of the given SQL query and the constraints upon the contents of the relevant SQL tables.  These axioms will all be passed as input to the Prover Engine.  Therefore, the axioms must be encoded in a language understood by the Prover Engine.  In the present case, this language is Prolog.  Using the assertions, a single proof goal will also be formulated.  The proof goal is built using both the axioms and the assertions.  As discussed earlier, if a postcondition asserts a formula >>phi>>, the proof goal will actually be formulated to try and satisfy >>NOT-phi>>.  If a satisfying assignment is found for >>NOT-phi>>, then we call this assignment a counterexample.  The counterexample demonstrates that >>phi>> cannot be guaranteed by the current SQL query.  In prose, the proof goal intuitively represents the following question:  in any model database that obeys the axioms of this SQL scenario, will the SQL resultset from this query ever satisfy >>NOT-phi>>?  Again, >>phi>> would represent a postcondition (a guarantee), and >>NOT-phi>> can be seen as a description of a flaw.  Finding a satisfying assignment for >>NOT-phi>> amounts to finding incontrovertible evidence of a flaw.  Failing to satisfy >>NOT-phi>>, on the other hand, means the query is guaranteed to always obey the postcondition.  This is why a result of "false" from the Prolog Prover Engine is indeed a happy outcome indicating proper behavior of the SQL code with respect to its specification.

Step 3 in Figure JJJ is accomplished simply by loading the axioms and goals into SWIPL Prolog and having Prolog resolve the goal.  Because of the negated approach, when Prolog outputs "false" this actually indicates to POQ that the desired properties of the query are valid.  In prose, this sounds backwards and counter-intuitive.  However, it will become very clear in the examples given later in the chapter.  To review the details regarding how Prolog makes a true/false judgement of a goal, refer to Chapter OOO.

Once POQ completes Step 3 from Figure JJJ, information begins flowing upward through the layers until it arrives at the UI for viewing by the user.  In case of a successful proof, the Prolog Prover Engine simply outputs "false," as explained in the preceding paragraph.  When that happens, it is trivial to make the Synthesis & Translation layer create a translation of the result into a more user-friendly message such as:  "the query is correct with regard to the given specification."  The more interesting case is when a counter-example is found that invalidates the specification.  The Prolog engine produces counter-examples that look like this:

    TABLE_X = [(a,b,22),(a,null,33)],
    TABLE_Y = [(a,null)]

There are at least two helpful transformations that the Synthesis & Translation layer could apply to such a result.  One is to display essentially the same information as shown above in Prolog, but to render it in a tabular format that is familiar to SQL users.  Another is to generate a complete SQL script that would create and populate SQL tables, run the original SQL query, and demonstrate the entire counter-scenario inside an actual RDB system (and outside of POQ).  Users could execute such scripts in an RDB system of their choosing.  Both of these potential transformations are shown in Figure LLL.  Again, the UI Layer is left unspecified in this thesis.  Therefore, no choice is made here as to whether the tabular counter-example and/or the counter-example SQL script would be displayed in a graphical window or printed at the command-line, or displayed or transmitted in any other manner.


??????????????????
??????????????????
    conclude this section. ui layer not treated. prover engine is SWIPL prolog. therefore, focus is now entirely on Synthesis & Translation layer.  According to Figure JJJ, the duties of ___ layer are 2a, 2b, and 4.  Step 4 has been covered sufficently.  Step 2b is ommitted due to the simplifying assumption.  Step 2a is the focus of the rest of this chapter.
??????????????????
??????????????????

=========================================================

In order to create and execute the sample proof goals illustrated in this chapter, SQL queries and CREATE TABLE statements have been manually transformed into Prolog axioms.  Nonetheless, all such transformations can and will be automated in a full implementation of POQ.  To support that claim, it must be demonstrated that the transformation is deterministic and algorithmic and suitable for automation.  This can be demonstrated by appealing to the concept of a SQL query tree.  Query trees are well understood and are covered extensively in textbooks and other database literature.  Many parsers exist that can generate a query tree from a SQL SELECT statement.

Producing a query tree from a SQL statement is similar to producing any arbitrary abstract syntax tree (AST) from source code in any computer language.  Query trees, however, exhibit a greater degree of regularity and simplicity that an arbitrary AST.  An example of a query tree is shown in Figure MMM.  All relational SQL operators are either unary or binary, which means that no operator node in a query tree will ever have more than two children.  Also, the closure property of relational algebra means that the application of any operator to its child relations always produces a relation as an output.  These guarantees on node count and output types guarantee a high degree of regularity in the tree's structure.

One SQL language construct that can add significant complications to a query tree is the correlated subquery.  Correlated subqueries are used in two upcoming examples in this chapter.  However, it remains an open question whether the manually-generated axioms in those examples are as suitable for automation as the axioms from the remaining four samples.

Another factor than could interfere with the simplicity of a query tree is coercion. (more explanation)

In the absence of correlated subqueries, generating the correct query tree from the SQL code inputted to POQ is all that is necessary to enable automated generation of the required axioms.  Once the query tree is known, the algorithm for generating the axioms can be built based on a post-order traversal of the (binary) query tree.  Coercions are presumed not to exist, so every node represents a relation (table).  Leaf nodes represent base tables, and each non-leaf node represents a single operation producing a derived table.  When a leaf node is visited, the algorithm uses the CREATE TABLE input to generate axioms governing the structure of this base table (and its tuples).  When a non-leaf node is visited, the algorithm relies on ready-made axiom schemas built into POQ.  These axiom schemas are discussed in detail later in this chapter.  At a non-leaf node, the appropriate axiom schema is chosen based on the operation associated with that node.  The axiom schema is instantiated using information from the child node (or child nodes) of the non-leaf node.  Once the post-order tree traversal completes, then all the required axioms have been generated.

Currently, the source code of the POQ project [cite:github] contains axiom schemas for five relational operators: JOIN, LEFT JOIN, WHERE, GROUP BY, UNION ALL.  Adding support for new operators requires formulating a new axiom schema for each newly-targeted operator.  Often it is not adequate to simply find a correct logical formulation.  Instead, internal idiosyncracies of the Prover Engine often necessitate finding a particular axiom formulation that plays to the strengths of the Prover's search algorithms and avoids the Prover's weaknesses.  In other words, there will be many axiom schemas that are logically correct and equivalent to one another, but the key to the practical applicability of POQ (or any similar tool) is the find the axiom schemas that are processed most efficiently by whichever prover engine is in place [cite:bjorner, possibly schumann?].

To further illustrate the readiness with which query tree traversal and axiom generation can be automated, the processing of a leaf node (base table node) and the processing of a non-leaf node (operator node) will now be shown in detail.  The processing is entirely straightforward; no complications arise that would prohibit automation.

>>>>>>>>>>>> leaf-node. table. tuples. domains. primary key on table.

>>>>>>>>>>>> Note: foreign key constraints involve more than one table. not yet handled in POQ.

A base table is always shown as a leaf node in a query tree, which is to say it has no further subordinate elements shown in the tree.  Nonetheless, a table can itself be broken down further into constituent parts.  A table definiton contains a list of column definitions.  Each column definition has a column name and a datatype.  POQ needs to examine all of these.

As mentioned near the top of this chapter, domain relational calculus provides a foundation for the axioms used in POQ.  When building the axioms that support a given query, the starting point is an individual domain.  When POQ reaches a leaf node, it examines the columns of the table represented by the leaf node, and determines which POQ domain corresponds to the SQL datatype of each column.

POQ begins with the domain of each column, and then creates axioms in a bottom-up fashion.  Tuple type axioms are built in terms of domain axioms.  Table axioms, in turn, are built in terms of tuple axioms.  Later, both the table axioms and the tuple axioms contribute to the instantiation of the operator axiom schemas at non-leaf nodes.

In order to represent each column type using Prolog predicates, POQ consults a lookup table that maps each SQL datatype to a POQ domain.  One simple case is to map SQL type "int unsigned" to the POQ domain natural_type.  All domains in POQ are finite domains.  For example, the following Prolog statements are used to define a finite domain 'natural_type,' which is a subset of the natural numbers.

natural_type(0).
natural_type(1).
natural_type(2).
natural_type(3).
natural_type(4).

The natural_type domain as just shown only has a cardinality of 5.  The cardinalities of the finite domains is something that POQ will allow to be configured at runtime.  For all examples in Chapter HHH, each domain used was of size ___.  Ensuring that domains are finite is a necessary step to prevent the Prolog proof procedure from taking an infinite branch of a proof tree.

Given the above specification for natural_type in Prolog, we can then specify a domain variable to range over natural_type by writing:

natural_type(X).

Any symbol beginning with a capital letter is always a variable in Prolog.  This capitalization rule is what allows us to unambiguously detect that 'X' is a variable in the preceding Prolog statement.  However, it is also what forces us to use lower-case names for some of the items that follow, such as 'null', even though in SQL we typically see this concept represented as 'NULL' and not 'null'.

Natural numbers are used in databases in many ways.  A column that represents, say, the number of cartons of roofing shingles in stock at a warehouse would likely use the natural number type.  Natural numbers are also often used as unique row identifier numbers in database tables.  In additional to natural numbers, another datatype that is used to uniquely label rows is the Globally Unique Identifier type, or GUID. We add this domain to our system as follows:

guid_type(fccy463).
guid_type(srce544).
guid_type(ddd213).
guid_type(tchc397).

Actual GUID values are longer than 'fccy463' and the other values shown above.  The choice of using shortened values to represent GUIDs is a deliberate design decision in our system.  To explain this decision, it is helpful to first consider the role that these values will play in the system.  To restate a prior point, the values that we use to populate the finite domain of each type are precisely the values that will later appear as attributes in concrete tuples.  However, in cases where assertions are proved valid, the end user of the proof system may never need to view any of the contrete tuples that were created during the proof process.  So in the case of a successful proof, it does not matter too much what our values look like, as long as they map closely enough to a real-life representation of the database so that we can have confidence in the proof.  In cases where assumptions are invalid, then our design requirements for these values become a bit more stringent.  As stated earlier, the detection of invalid statements will cause the system to output a counterexample.  The counterexample is an assignment of values to each variable that was featured in the assertion.  Recall that the variables in the assertion can represent table values, row values, and parameter values (in cases where the SQL statement is parameterized).  This means that the system's GUID values--such as 'fccy463'--will be shown to the user as part of a concrete counterexample.

Depending on whether we expect our end user to have a flexible imagination or not, we might accept or reject certain design criteria in defining our GUID domain.  The design criteria that have been applied here are simple.  When a GUID is printed out as part of a counterexample, our user must recognize this value as representing a plausible potential value that might someday get inserted into the pertinent database table.  The value does not need to be an actual value that is currently present in the real-life database.  Quite the contrary, because if it did need to be present currently, then our tool would only check the satisfiability or unsatisfiability of a SQL statement given the current database state, and that is not the intent of the tool at all.  The intent of the tool is to prove that certain properties of the SQL statement and its result set will hold true for all states that this database can attain, including past states, present states, and future states.  The user is expected to understand that 'fccy463' represents something that might someday appear in the database.  Even when the user knows that the appearance of 'fccy463' in a counterexample in no way indicates that 'fccy463' is present in the real-life database currently, the user could still raise objections.  One such objection would be that 'fccy463' is not formatted appropriately the way that real-life GUIDs are.  This is where the expectation of a "flexible imagination" comes into play.  It seems reasonable to expect that most trained users could easily know to "mentally map" a symbol like 'fccy463' into a realistic GUID such as '{5224F545-A443-4859-BA23-7B5A95BDC8EF}' in order to visualize the real-life equivalent of the counterexample, and in order to generate a real-life SQL script filled with test data for testing the counterexample, if needed.  The idea that our users will be flexible enough to accept "reasonably recognizable" values allows us to keep the Prolog code simple, and allows us to avoid any potential hassles in cases where a true-to-life representation of a datatype might involve the use of characters or strings that are difficult to work with in Prolog.

If there are any enum types in the database, then we can easily define such a type in our Prolog system:

color_enum_type(red).
color_enum_type(orange).
color_enum_type(yellow).
color_enum_type(green).

The most important type remaining is the string type.  The introduction of that type will involve another discussion of design trade-offs.  Therefore, before we consider strings, we shall cover the simple technique of supporting nullable attributes in our system.  We only need one way to represent NULL, so we will use the Prolog constant 'null.'  Constants must begin with lower-case letters in Prolog, so we are unable to use a constant that would faithfully coincide with the capitalized keyword that SQL uses.  Here we declare our constant as the only thing that is null:

isnull(null).

Then, we must add clauses that will allow 'null' to appear any place where a number, GUID, or enum value can appear.  This is done as follows:

natural_type(null).
guid_type(null).
color_enum_type(null).

Ignoring the special case of a primary key column, the default behavior in SQL table creation (double-check standard. cite section of standard?) is to create each table column such that NULL can appear in any column, no matter the datatype of the column.  This observation is what motivated the inclusion of our 'null' constant into each domain of our system.  However, it is also possible to override the default nullability in SQL and declare columns that do not permit NULL.  To accomodate this possibility, we add the following shorthand:

not_null(X) :- \+isnull(X).

Instead of using the above declaration of not_null, we could simply type '\+isnull' wherever a predicate like not_null would appear.  However, the shorthand is easier on the eyes, and it more closely resembles the SQL syntax that it represents (i.e. 'NOT NULL').

So in order to add a string domain to the system, we know that we will add something like 'string_type(null)' in order to support nullable string columns.  After that, however, the possibilities are vast regarding what other items should become members of this finite domain.  In the earlier discussion of GUIDs, it was deemed reasonable to expect that a user would accept 'fccy463' as representing a GUID, even if it is an imperfect representative.  However, while we could also request that our users accept things like 'ejhrbxjs' or 'abcdef' as reasonable product names or employee names, at best this seems like a significant distraction.  At worst, it could be a significant obstacle to the user's ability to comprehend some counterexample assignments produced by the system.

Therefore, for usability reasons, the system will actually support multiple string domains such as name_string_type and product_string_type.  This way, a counterexample involving a Product table would display values like "desk," "book," "bottle," or "car" instead of gibberish strings.  To pay the cost of this usability feature, however, a burden of extra annotations must be imposed.  When the system parses SQL table definitions, it will no longer be sufficient for the system to recognize a SQL type declaration as "some string type."  Instead, a specific annotation should be provided so that the proof system can be alerted to map the column's type specifically to name_string_type, product_string_type, or some other specific subset of string representatives.

Now that some basic types have been defined, it will be possible to define tuples composed of these types.  While it was easy to list some helpful datatypes without considering any particular database, it is not possible to populate our system with tuple types in the same (free-form? ad-hoc? open-ended?) way.  The set of datatypes available in SQL databases are fairly uniform across the different database products.  Therefore, other than the idea of tailoring our string types to look sensible in particular scenarios, we can prepopulate our system with several finite domains that will likely be useful for proofs involving any database we encounter.  The same cannot be said for tuple types.  The tuple types that we will require for proofs about a given database are likely to be unique to that particular database.  Therefore, concrete tuple types are something that the proof system will have to generate after parsing the 'CREATE TABLE' statements that define the given database.

To give an example of a tuple type specification in the proof system, we need a concrete table defintion as input.  Consider the following table defition:

CREATE TABLE Product ( product_id int, product_name varchar(100), PRIMARY KEY (product_id) );

From the table definition, it can be determined that the tuple will have two components: one numeric component and one string component.  The string component will have the default nullability.  However, because the numeric component is also the table's primary key in this case, SQL will automatically disallow it from ever containing NULL.  Based on that, the system can create the following tuple type definition:

product_tuple( PROD_ID, PROD_NAME ) :-
    natural_type(PROD_ID), not_null(PROD_ID),
    product_string_type(PROD_NAME).

The above is a logical formula in Prolog that states that any sequence of variables ( PROD_ID, PROD_NAME ) is indeed a product_tuple as long as PROD_ID is a natural number value and not null, and PROD_NAME is a valid string.  In the case of PROD_NAME, since the nonnull predicate has not been asserted for that variable, we would also allow the sequence ( PROD_ID, null ) to be a product tuple, as long as the requirements on PROD_ID are met.

Of course, rather than asserting that "if requirements are met, then ( PROD_ID, PROD_NAME ) is a product tuple," we would really like use an "if and only if" (bi-implication) instead of a simple "if" (implication).  Prolog does not have a symbol for logical bi-implication.  However, if no other implication is provided that can imply that ( PROD_ID, PROD_NAME ), then we have essentially achieved bi-implication due to the behavior of Prolog.  Prolog will only ever allow the the product_tuple conclusion to be true when Prolog can satisfy the body of a concrete clause where the head symbolizes the satisfaction of the product_tuple predicate.  The product_tuple clause shown above would be the only one our system would generate with product_tuple as the head, so this would be the only way allowable to conclude that some sequence ( PROD_ID, PROD_NAME ) is a permissible product tuple.

Now that tuples have a place in the system, the only remaining structure to define is the table types that are formed from collections of tuples.  A SQL table is an approximation of a relation.  A relation in a strict sense is an unordered set of tuples.  Note that the definition of set means that a relation will never contain duplicate tuples.  In a SQL database, on the other hand, it is possible to create tables that contain duplicate rows.  Additionally, a SQL database usually only provides a very imperfect sense of the supposed lack of ordering over rows.  The database will typically display a result set in the same order no matter how times a query is run, and many times the resulting rows will appear to have been sorted by some numeric column, even when the query did not include any keywords that would require an order.

To maximize the kinds of SQL query bugs that our system can catch, we need to emulate the non-set characteristic of SQL tables that admits duplicate rows in a table.  Otherwise, if our system obeyed a faithful set-based description of relations, then the proof system might uphold a user's assertion about the uniqueness of some row, only to be disappointed when contrary results are observed in the real-life database.  Perhaps counter-intuitively, a very different rationale will apply to making design decisions about whether to emulate SQL's tendency to obey some apparent ordering when no ordering is required.  In the case of SQL's "apparent" result ordering, we would provide a more valuable service by deliberating seeking to avoid emulating this behavior.

The argument in favor of avoiding emulation of apparent ordering is based on the observation that such ordering can change on a moment's notice, sometimes after being stable for months or years.  This almost always causes unfortunate side-effects whenever a SQL user had been lulled into relying on the ordering.  When inherently unordered queries produce "apparently ordered" results, this is usually because the internal algorithms of the RDB store and retrieve data relying on fast-lookup indices.  If a database administrator decides to drop an index or reformulate an index, then the RDB might suddenly be caused to answer a long-standing query using a different retrieval order than before.  This is virtually certain to cause application errors if the application had been designed to attach some meaning or ranking based on the order of the output rows from the query.  The SQLite RDB contains an interesting feature to help users detect such errors.  The SQLite command 'PRAGMA reverse_unordered_selects;' affects queries that do not contain any special keywords that require the sorting of results.  When the pragma is enabled, the RDB will output result sets for such queries in the *reverse* order of whatever the retrieval algorithm normally would have done.  By alternating whether this pragma is enabled or disabled, SQLite users have a better chance of noticing where faulty presumptions of sorting have led to bugs in their application.

===============
= the next paragraph is out-of-date. needs fixing to match latest POQ implementation
==============
The preceding discussion establishes the relevant properties that our table types should possess.  Designing a structure that admits duplicate tuples is not difficult.  However, attempting to provide a true sense of the absence of any ordering is potentially trickier.  A list structure obviously has an order, so using the built-in Prolog list type to express SQL tables would at first glance seem problematic.  Nevertheless, Prolog lists are the chosen method in our system.  Thankfully, it turns out that the nature of Prolog's execution engine neutralize any concerns over undesirable ordering.  In our Prolog-based proof system, the backtracking behavior of Prolog's proof search will be sufficient to ensure avoidance of any "apparent" (yet unguaranteed) ordering.  Concretely, if an assertion needs to be validated for a set of product tuples regardless of the order in which they appear, Prolog will eventually test many possible reorderings via backtracking.

We continue to use the same 'CREATE TABLE' declaration that was shown above for the Product table.  The table has a primary key, so we will eventually need to enforce uniqueness on the product_id column.  Leaving that aside for now, we begin by declaring a typed list.  As a first pass, we simply provide two clauses to define a list of tuples that can only contain product_tuple typed tuples.

product_table( [] ).

product_table( [(P_ID,P_NAME)|T] ) :-
        within_table_size_limit([(P_ID,P_NAME)|T]),
        product_tuple(P_ID, P_NAME),
        product_table(T).

These clauses state that an empty list is a valid representation of a product table.  Also, a list such that the head of the list is a product tuple and the tail of the list (recursively) meets the definition of product_table is also itself a product table.  Note that a restriction is also being imposed on the size of the table by using the predicate within_table_size_limit.  We will return to the topic of restricted table sizes in a moment.

The previous two clauses for product_table do not account for the primary key column at all.  In practice, most tables do possess a primary key column (although a few beleaguered heirs to legacy systems might beg to differ).  Therefore, in most cases the type of recursive table definition just shown will not be sufficient.  For the Product table under development, we actually use a predicate named product_table_with_constraints which is more than twice as long as the product_table code just shown.  The predicate product_table_with_constraints is also defined recursively.  The full source code can be found in Appendix AAAA.

At this point it is necessary to acknowledge and explain a few limitations of the proof system that have become evident in this section.  We stated several times that each domain is defined as a finite domain.  Also, the table definitions use a predicate named within_table_size_limit to enforce an upper bound on table size.  Placing limits on domain size and on table size weakens the proofs that are emitted, and requires us to clearly label each proof with the boundaries that were in effect during validity testing.  For example, it would not be appropriate for the system to state simply that an assertion was "proved valid."  Instead, the only appropriate claims will be those of the format: "assertion was proved valid in a database where table sizes cannot grow beyond 20 rows, and where the numeric domain ranges from 0 to 20, and where there are never more than 15 unique strings stored throughout the database at any time."

These restrictions may sound severe, but there is good reason to believe that they actually do not dampen the usefulness of such a prover tool in any significant way.  For queries that reference only a handful of tables, flaws in the query formulation can typically be easily demonstrated using counterexamples that are populated with only small-sized tables.  Therefore, it will normally be efficient to run the first few proof attempts using tight bounds on domain size and table size.  If those proofs succeed, yet more confidence in the correctness of the SQL is still desired, then the bounds can be incrementally lifted as more and more proofs are attempted using larger and larger table definitions.  This incrementally widening approach can be continued either until sufficient confidence is gained, or until the proof task becomes intractable.  Furthermore, it is definitely not necessary for our system to represent infinite domains or tables of infinite size.  If the only counterexample to the correctness of some assumption were a counterexample that relied on an infinite table, then this would not serve any practical application.  In practice, all databases are finite, with attributes ranging over finite domains.

In other words, the size restrictions are notable but not overly troubling.  However, the system will have to perform bookkeeping to make sure that the sizes of the domains are large enough to populate any maximum-sized table that may have unique constraints on each of its columns.  Similarly, some reasonable heuristic may need to be devised so that table sizes are suffiently large so as to not artificially impede a given JOIN from returning a non-empty result.  The most conservative heuristic would be to prevent any derived table resulting from a JOIN from exceeding a maximum size that is the square of the currently imposed maximum size of any base table.  Here, too, however, a smaller maximum could be applied first, keeping in mind that an incremental widening approach can always be tried afterward.  As other operations become supported, then similar remarks would likely apply for operations such as INTERSECT, EXCEPT, and others.

The use of finite domains for each attribute raises another issue beyond weakened proof power.  The other issue pertains to what might be called the "degree of query awareness" that our tool can (or cannot) achieve while using small finite domains for attributes.  The researchers who created the ADUSA tool for testing RDB query engines sought to avoid a particular problem of non query-aware approaches.  They noted that any tool that generates random table content for the purpose of running test queries will have little chance of producing non-empty result sets if the query to be targeted for testing is:

SELECT * FROM Student WHERE name = 'Amelia Barnett';

No matter how many times the Student table is filled with random test data, it will be highly unlikely that a random generator type of tool would ever put "Amelia Barnett" into the Student table.  Therefore, the test query might be executed many times, but the test framework would never be able to ascertain anything particularly meaningful about the characteristics of the data that this query might potentially return.

The Prolog-based tool proposed in this thesis would likewise never simulate any table containing the precise name "Amelia Barnett."  However, our Prolog-based tool can do something just as useful, as long as the SQL query to be tested is formulated and annotated properly.  For our system, the query should instead be formulated like so:

SELECT * FROM Student WHERE name = @name_parameter;

Then, part of the precondition annotations should state that @name_parameter must be some string that is present in the Student table, or else that @name_parameter must be some string that is *not* present in the Student table.  It seems that the only motivation for running tests on the "Amelia Barnett" query would be to see how the query behaves when "Amelia Barnett" is located, and/or how the query behaves when "Amelia Barnett" is *not* located.  As long as the motivation is as just described, then our "@name_parameter" approach would perform verification that should be equally as meaningful as that of any "query-aware" system.  The only difference would be that in our system some other name would be chosen from our name_string_type domain.  On the other hand, there may be some rare case where a user wished to specifically execute a query containing "Amelia Barnett" due to some suspicious of an obscure bug in the RDB related to precisely that string.  In that case, asking our Prolog tool to test the query is pointless, since there is very little similarity between the internals of the Prolog tool and the internals of whatever real-life RDB is suspected of having the obscure bug.

Figure NNN shows all datatype mappings used in this chapter.

Domains are built into POQ (see Appendix TTT).  It is easy to add more domains as needed.  A principle that is widely adhered to among SQL practicioners is the idea that each column position in a tuple can only contain an atomic, indivisible item.  Therefore, it is unlikely that POQ would ever need to define domains to represent any complex datatypes such as trees or arrays.  On the other hand, some applications do store XML snippets in BLOB-typed columns of SQL tables.  Support for column types of non-atomic varieties is not planned for POQ.

Domains are built-in, but specific tuple types are not.  The tuple types POQ requires will potentially be different for each SQL query POQ is tasked with processing.  These types, therefore, must be generated during the post-order traversal of the query tree during each new run of the tool.  The same is true for the table axioms.  Of course, if the user knows in advance that he/she will be processing many SQL queries that all access the same database, then it might be reasonable to have POQ generate up front all the tuple types and table axioms for all base tables in the database.  Those axioms could then be cached for reuse during the processing of each query.  However, the time saved by this up-front processing would likely be negligible.  On the other hand, the user may appreciate only having to provide DDL input once, rather than providing the CREATE TABLE declarations each time a new query is input.

>>>>>>>>>>>> internal node. determine operator. determine child node info.

Now for an illustration of processing a non-leaf node.  This illustration is for a node representing JOIN operator.  Nodes for this operator always have two child nodes.  JOIN is one of ???5??? operators currently implemented as Prolog axiom schemas.  Full details for all ???5??? supported operators can be found in Appendix UUU.

Arguably the most popular operation in SQL is the JOIN operation, so any tool for analyzing SQL would be well-advised to make support for JOINs job number one.  The necessary Prolog axioms for formalizing JOIN operations were devised early on in the development process.  However, the design of these axioms has undergone one major change since their initial creation.  Initially, it seemed that a reasonable design goal for this prover tool would be to only require the most straightforward parsing of the SQL queries.  The goal was to translate the SQL into an equivalent set of FOL formulas in the simplest and most direct way possible.  In other words, a front-end parser for the tool would not be required to do any re-writing or optimizing of the initial query tree generated via simple parsing.

Based on those early design goals, the original FOL axioms for JOIN involved statements to specify a cartesian product of the relevant tables, followed by statements to describe a filtering of the cartesian product results using the join condition as the filter expression.  It is well-known that such a naive approach to JOIN implemenation does not perform well in any RDB query execution engine.  As it turns out, this approach did not perform well in a prover setting either.  Therefore, the current axiomatization of JOIN operations in this system is more analogous to a nested loops approach, where each row in the outer-loop table is compared to each row in the inner-loop table, and only when comparison succeeds are the row pairs allowed as members of the result set.

Because each new JOIN can use a new pair of tables and a new JOIN condition, the prover system will have to dynamically generate clauses to describe each particular JOIN.  However, there is a template that the system can follow, so this process can in principle become fully automated someday.  Figure III shows a sample set of axioms to define the JOIN indicated by the SQL fragment: ShoppingCart c JOIN CartDetail cd ON c.cart_id = cd.cart_id.  In addition to the JOIN snippet just provided, it is also necessary to understand the tuple types involved in order to make complete sense of what is being declared.  The tuple types in this case are "shopping_cart_tuple(CART_ID,CUST_ID)" and "cart_detail_tuple(CART_ID, PRODUCT)."  The CART_ID variable ranges over the domain of GUIDs; the CUST_ID variable ranges over the domain of natural numbers; and the PRODUCT variable ranges over the strings in product_string_type.

The remainder of this section will be devoted to discussion of these axioms for the JOIN operation.  This operation has the largest set of axioms of any operation in the system.  Also, the properties of these axioms are representative of properties contained within the axioms for aggregation and antijoin used elsewhere in the system.  The source code for the entire current system can be found in Appendix AAAA.





% ----------------------------------------------------------

/*
There are 7 different clauses to express sc_join_cd_on_EXPR.

There should be no duplication in outcomes due to careful management
of when each of the 7 clauses is allowed to be applied.

Each one of the 7 handles a NON-OVERLAPPING subset of cases based on
the SIZE of the first two list variables.

The cases (by size of the two lists) are:

[]    []
1+    []
[]    1+
1     >1
1+    1     (1+ means 'one or more')
2+    2+  ... and the first list size is greater to or EQUAL to the second
2+    2+  ... and the first list size is LESS THAN the second
*/

meets_join_sc_cd(CART_ID_sc,_CUST_ID,CART_ID_cd,_PRODUCT) :-
        CART_ID_sc = CART_ID_cd.


sc_join_cd_on_EXPR( [], [], [] ).


sc_join_cd_on_EXPR(
  [(CART_ID_sc,CUST_ID) |L2T],
  [],
  [] ) :-

        shopping_cart_table([(CART_ID_sc,CUST_ID)   |L2T]),

        within_table_size_limit([(CART_ID_sc,CUST_ID)   |L2T]).


sc_join_cd_on_EXPR(
  [],
  [(CART_ID_cd,PRODUCT)   |L2T],
  [] ) :-

        cart_detail_table([(CART_ID_cd,PRODUCT)   |L2T]),

        within_table_size_limit([(CART_ID_cd,PRODUCT)   |L2T]).



% single cart but longer list of c_details, MEETS JOIN conditions
sc_join_cd_on_EXPR(
  [(CART_ID_sc,CUST_ID)   |[]],

  [(CART_ID_cd,PRODUCT)   |L2T],

  [(CART_ID_sc,CUST_ID,CART_ID_cd,PRODUCT) |R] ) :-


        shopping_cart_tuple(CART_ID_sc,CUST_ID),

        cart_detail_table([(CART_ID_cd,PRODUCT)   |L2T]),

        length([(CART_ID_cd,PRODUCT)   |L2T],X),
        X>1,
        within_table_size_limit(L2T),

        meets_join_sc_cd(CART_ID_sc,CUST_ID,CART_ID_cd,PRODUCT),

        sc_join_cd_on_EXPR([(CART_ID_sc,CUST_ID)   |[]] , L2T, R ).


% single cart but longer list of c_details, FAILS TO MEET JOIN conditions
sc_join_cd_on_EXPR(
  [(CART_ID_sc,CUST_ID)   |[]],

  [(CART_ID_cd,PRODUCT)   |L2T],

  R ) :-

        shopping_cart_tuple(CART_ID_sc,CUST_ID),

        cart_detail_table([(CART_ID_cd,PRODUCT)   |L2T]),

        length([(CART_ID_cd,PRODUCT)   |L2T],X),

        X>1,
        within_table_size_limit(L2T),

        \+meets_join_sc_cd(CART_ID_sc,CUST_ID,CART_ID_cd,PRODUCT),

        sc_join_cd_on_EXPR([(CART_ID_sc,CUST_ID)   |[]] , L2T, R ).



% longer carts list but SINGLE detail item, MEETS JOIN conditions
sc_join_cd_on_EXPR(
  [(CART_ID_sc,CUST_ID)   |L2T],

  [(CART_ID_cd,PRODUCT)   |[]],

  [(CART_ID_sc,CUST_ID,CART_ID_cd,PRODUCT) |R] ) :-


        shopping_cart_table([(CART_ID_sc,CUST_ID)   |L2T]),

        cart_detail_tuple(CART_ID_cd,PRODUCT),

        within_table_size_limit(L2T),

        meets_join_sc_cd(CART_ID_sc,CUST_ID,CART_ID_cd,PRODUCT),

        sc_join_cd_on_EXPR( L2T, [(CART_ID_cd,PRODUCT)   |[]] ,   R ).


% longer carts list but SINGLE details item, FAILS TO MEET JOIN conditions
sc_join_cd_on_EXPR(
  [(CART_ID_sc,CUST_ID)   |L2T],

  [(CART_ID_cd,PRODUCT)   |[]],

  R ) :-

        shopping_cart_table([(CART_ID_sc,CUST_ID)   |L2T]),

        cart_detail_tuple(CART_ID_cd,PRODUCT),

        within_table_size_limit(L2T),

        \+meets_join_sc_cd(CART_ID_sc,CUST_ID,CART_ID_cd,PRODUCT),

        sc_join_cd_on_EXPR( L2T, [(CART_ID_cd,PRODUCT)   |[]] ,   R ).



% adding one more details item to an 'already crossing'
sc_join_cd_on_EXPR(
  [(CART_ID_sc,CUST_ID)   |L1T],

  [(CART_ID_cd,PRODUCT)   |L2T],

  FINAL ) :-

        shopping_cart_table([(CART_ID_sc,CUST_ID)   |L1T]),

        cart_detail_table([(CART_ID_cd,PRODUCT)   |L2T]),

        length([(CART_ID_sc,CUST_ID)   |L1T],  X),
        X>1,
        length([(CART_ID_cd,PRODUCT)   |L2T],  Y),
        Y>1,
        X>=Y,
        sc_join_cd_on_EXPR([(CART_ID_sc,CUST_ID)   |L1T],    L2T,      POUT),
        sc_join_cd_on_EXPR([(CART_ID_sc,CUST_ID)   |L1T], [(CART_ID_cd,PRODUCT)   |[]],  MOUT),
        merge(POUT,MOUT,FINAL).


% adding one more cart to an 'already crossing'
sc_join_cd_on_EXPR(
  [(CART_ID_sc,CUST_ID)   |L1T],

  [(CART_ID_cd,PRODUCT)   |L2T],

  FINAL ) :-

        shopping_cart_table([(CART_ID_sc,CUST_ID)   |L1T]),

        cart_detail_table([(CART_ID_cd,PRODUCT)   |L2T]),

        length([(CART_ID_sc,CUST_ID)   |L1T],  X),
        X>1,
        length([(CART_ID_cd,PRODUCT)   |L2T],  Y),

        Y>1,
        X<Y,
        sc_join_cd_on_EXPR(L1T,  [(CART_ID_cd,PRODUCT)   |L2T],   POUT),
        sc_join_cd_on_EXPR([(CART_ID_sc,CUST_ID)   |[]],   [(CART_ID_cd,PRODUCT)   |L2T],    MOUT),
        merge(POUT,MOUT,FINAL).


% ----------------------------------------------------------





This chapter has described a feasible implementation for the POQ tool.  The following chapter demonstrates the usefulness and practical applicability of POQ by applying POQ's techniques to six concrete examples.
